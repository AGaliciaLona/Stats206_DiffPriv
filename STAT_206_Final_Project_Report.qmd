---
title: "STAT 206 Final Project Report"
author: "Axel Galicia Lona, Linlin Liu, and Nancy Lopez"
date: "2024-12-13"
echo: true
format:
  html:
    toc: true
    toc-location: left
    embed-resources: true
    code-line-numbers: true
  pdf:
    code-line-numbers: true
editor: 
  markdown: 
    wrap: 72
---

# Background and Motivation

With rapid population expansion and technical improvement, the movie
industry grows increasingly sophisticated nowadays. Therefore, studios,
producers, and investors need reliable predictions to make informed
decisions about budgeting, marketing, and resource allocation.

As viewers, we are also interested in the factors that contribute to a
film's revenue, because these generally define its success. Furthermore,
the highly competitive nature of the film industry, as well as the huge
economic impact of movie income, motivated us to pursue this project.
Thus, we chose to build a multiple linear regression (MLR) model for our
project.

Differential privacy (DP) is essential for datasets containing sensitive
aspects of movies, as it ensures that individual data points cannot be traced
back to their sources. By adding controlled random noise to the data,
differential privacy balances utility with confidentiality, enabling
meaningful analysis without exposing proprietary or personal information and
making it a crucial tool in this data-driven exploration.

# Hypotheses

Based on the information provided by the data set, we propose the following hypotheses:

1. Different attributes of a feature movie significantly impacts its revenue.

2. A movie’s budget and genre contribute more significantly than others.

# Final MLR Model

After the model buildling process, our proposed final MLR model is

$$Y = 2.6366e^{-5}X_1 + 8.5311e^{-1}X_2 + 3.3198X_3 - 7.7753e^{-1}X_4 +
1.3633e^{-4}X_5 + 5.5155X_6 + \epsilon$$

where

- $Y$ = *lnrev*

- $X_1$ = *budget*

- $X_2$ = *popsqrt*

- $X_3$ = *genres*

- $X_4$ = *popsqrt* * *genre*

- $X_5$ = *vote count*

- $X_6$ = *original language*

The assumptions of $\epsilon$ are as follows:

- linearity: a linear relationship exists between the response and predictor
variables

- constant variance: the variability about the least squares line is
generally constant

  - VAR($\epsilon_i$) = $\sigma^2$, where $i$ = 1, 2, ..., n
  
- normality: the distribution of the residuals is approximately normal

- independence: the residuals are independent from each other

  - for $\epsilon_i$ and $\epsilon_j$, $i \neq j$, where $i$ = 1, 2, ..., n,
  and $j$ = 1, 2, ..., n
  
In order to better observe specific predictors, we first converted categorical
predictor variables into binary integers. We assigned 1 to "Drama" and "en" and
0 to "Comedy" and "fr."

We utilized scatter plots to examine the relationship between each predictor
variable and the response variable. We observed distinct patterns in *runtime*,
*vote_average*, and *vote_count*. The scatter plots for *budget* and
*popularity* did not reveal a clear linear relationship or direction.

# Introduction To Differential Privacy

DP is a statistical framework designed to safeguard individual privacy in
datasets. By introducing random noise to analysis outputs, DP ensures that no
single individual’s data can be identified or inferred, even with additional
knowledge of the dataset. This approach balances privacy protection with the
need for accurate data analysis, enabling organizations to derive insights
without compromising privacy.

The formal definition of DP states that for any 2 datasets, D and D'
differing by one individual data point, the probability of mechanism $M$
producing a certain result should be very close for both datasets. This
can be mathematically expressed as:
$Pr(M(D)\in R)\leq exp(\epsilon)*Pr(M(D') \in R)$ where $\epsilon$ is
the privacy parameter. A small privacy parameter means strong privacy
since the output reveals less information about individual data. By
having two similar datasets with one missing a data point, it becomes
extremely difficult to guess whether a specific persons data was used,
hence protecting individuals' privacy.

Formally, DP guarantees that for any two datasets, D and D', differing by one
data point, the probability of a mechanism $M$ producing a specific output
remains similar. Mathematically: 
$Pr(M(D)\in R)\leq exp(\epsilon) \cdot Pr(M(D') \in R)$, where $\epsilon$ is 
the privacy parameter, with smaller values indicating
stronger privacy. This ensures that whether or not a specific individual's data
is included, the outputs are nearly indistinguishable, protecting privacy
effectively.

An effective noise-adding mechanism maintains data utility while preserving
privacy, allowing meaningful analysis without risking sensitive information.
In an era of widespread data collection, DP is vital for ensuring privacy while
enabling technological and analytical advancements.

# Differential Privacy Applications

DP introduces mathematical noise to the data, ensuring that insights can
be derived without revealing specific details about any individual
involved. This balance between utility and privacy has made DP an
essential tool in modern data-driven industries.

Tech companies like Apple are pioneers in integrating DP into their
services. For example, Apple uses DP in features like QuickType ( Apple
differential privacy technical overview, n.d.), where it analyzes typing
patterns to improve text prediction and autocorrect functionality. This
ensures no sensitive typing data is accessible to the company while
still enabling advanced machine learning models to learn user behaviors.

Governments, including the U.S. Census Bureau, rely on DP as well to
protect participants' confidentiality. In the 2020 Census, DP was
introduced to anonymize demographic data shared with policymakers,
researchers, and the public (Differential privacy and the 2020 census,
2021). Adding noise to the reported statistics ensures that privacy is
upheld while still providing accurate and actionable population trends
for resource allocation, urban planning, and legislative redistricting.

Another field that widely employs DP is the healthcare sector. Hospitals
and research institutions can share anonymized patient data for
large-scale studies, like predicting disease outbreaks or evaluating
treatment efficacy. DP ensures that even if the dataset is intercepted,
it is mathematically improbable for attackers to extract identifiable
patient information.

By enabling secure data analysis, DP empowers industries to innovate
without eroding public trust. This is especially critical in a world
increasingly reliant on data for decision-making, where user privacy is
both a legal mandate and an ethical responsibility.

# Noise Implementation Methods

The three primary noise mechanisms for DP are Gaussian, Laplace, and
Exponential. This project uses the Laplace mechanism, which provides
$(\epsilon, 0)$-DP by adding noise from a Laplace distribution:
$X \sim \text{Laplace}(0, \Delta f / \epsilon)$. Here, $\Delta f$ is the
query sensitivity, representing the maximum change caused by adding or
removing a single data point, and $\epsilon$ is the privacy budget. A smaller
$\epsilon$ increases noise for stronger privacy, while a larger $\epsilon$
reduces noise for better accuracy. The Laplace mechanism works best for
numerical queries with low to moderate $L_1$ sensitivity but is unsuitable for 
high-sensitivity or non-numerical queries.

The Gaussian mechanism, offering $(\epsilon, \delta)$-DP, adds noise sampled
from a Gaussian distribution: $X \sim N(0, \sigma^2)$. The noise
standard deviation $\sigma$ is determined by $\Delta f$, $\epsilon$,
and $\delta$, a relaxation parameter, using
$\sigma = \frac{\Delta f \cdot \sqrt{2 \ln(1.25/\delta)}}{\epsilon}$. Gaussian
noise is particularly effective for queries where $L_2$ sensitivity is smaller
than $L_1$ sensitivity, as it introduces less noise. While the mechanism is
suitable for high-sensitivity or large datasets, its privacy guarantees are
slightly weaker than the Laplace mechanism.

The Exponential mechanism applies to numerical and categorical queries by
selecting an output based on a probability distribution weighted by a utility 
function. The probability of selecting output $r$ is proportional to 
$\exp\left(\frac{\epsilon \cdot u(D, r)}{2 \Delta u}\right)$, where $u(D, r)$
is the utility function and $\Delta u$ is its sensitivity. It is ideal for
scenarios requiring non-numeric outputs, such as ranking or parameter
selection. However, it relies heavily on defining a suitable utility function
and may perform poorly for high-sensitivity queries.

# diffpriv() Package

We implemented DP to the final MLR model we built using the R package
*diffpriv()*. This package simplifies DP implementation by providing tools
like genetic mechanisms and a sensitivity sampler, which estimates sensitivity 
empirically instead of calculating exact bounds. This approach is
computationally efficient, especially for complex queries or large datasets.

DP noise is added to query results based on their sensitivity. Using
*diffpriv()*, the sensitivity sampler estimates sensitivity directly from
sample data, bypassing complex calculations and delivering practical and
accurate results for most queries.

To use the package, the process begins by defining the target function for 
privatization, such as a dataset, mean, or regression coefficients. For our
project, we privatized linear regression coefficients. Sensitivity was
determined using the Laplace distribution, with global sensitivity calculated
as $\frac{1}{n}$, where $n$ is the number of observations in the MLR model.
Larger datasets reduce the impact of individual data points, requiring less
noise and improving efficiency while maintaining privacy.

The privacy budget $\epsilon$, which balances privacy and accuracy, was set to
1, offering moderate privacy. After defining privacy parameters, the
*DPMechLaplace()* function added Laplace noise to query results, and the
*releaseResponse()* function released the privatized output. This final output 
protects individual privacy while ensuring the data's utility for third-party 
analysis.

# Implementation of diffpriv()

The Laplace mechanism, a widely used approach for DP, was employed to protect
the outputs of our final MLR model, which estimated the relationship between
log revenue (*lnrev*) and predictors including *budget*, *popsqrt* * *genre*,
*o_lang*, and *vc*. The model's coefficients, including the intercept, formed
the query output to be privatized. Adding Laplace noise to these coefficients
ensured that individual data points remained secure.

The sensitivity $\Delta f$ and the privacy budget $\epsilon$ were set to $1$.
This balanced privacy and accuracy, with smaller $\epsilon$ values increasing
noise for stronger privacy and larger values reducing noise for better accuracy.

The query output dimension was $7$, encompassing one intercept and six
predictors. Noise was applied to each coefficient, preserving privacy without
compromising the overall utility of the model.

```{r,eval=FALSE}
library(diffpriv)

set.seed(111)  # for reproducibility

# define target function for linear regression coefficients with intercept
target_function <- function(X) {
  model_sim <- lm(lnrev ~ budget + popsqrt * genre + o_lang + vc, data = X)
  return(coef(model_sim))  # returns coefficients including the intercept
}

# define the Laplace mechanism
# the sample size is 2624
# dims = 7 because there's 1 intercept and 6 variables
mechanism <- DPMechLaplace(target = target_function, sensitivity = 1 / 2624,
                           dims = 7)

# set privacy parameters
privacy_params_sim <- DPParamsEps(epsilon = 1)

# generate private response for one run
private_response <- releaseResponse(mechanism,
                                    privacyParams = privacy_params_sim,
                                    X = sub3)

# display non-private and private coefficients for comparison
cat("Non-private coefficients:\n", coef(lm(lnrev ~ budget + popsqrt * genre + o_lang + vc, data = sub3)), "\n")
cat("Private coefficients:\n", private_response$response, "\n")
```

We then conducted a simulation study involving 1,000 iterations to evaluate the
impact of the Laplace mechanism on the coefficients. The objective was to
assess how the mechanism's added noise affects the coefficients' distribution
and compare the results to the non-private regression coefficients.

## Simulations

In each iteration, the Laplace mechanism was applied to the regression
coefficients, adding noise proportional to the sensitivity of the query
and inversely proportional to the privacy budget ($\epsilon$). Using
*releaseResponse()*, we generated a set of private coefficients for each
simulation. These private coefficients were stored for further analysis.

A standard MLR model was fitted to the dataset to compute the non-private 
coefficients. These served as a baseline for comparison and were calculated
once since they remain constant across simulations.

## Visualizations

Density plots were created to display the distributions of the private
coefficients across the 1,000 simulations. Vertical lines were added to
indicate the corresponding non-private coefficients, providing a comparison of
their centrality and variability. Each coefficient was plotted in a separate
facet for clarity.

```{r, eval=FALSE}
set.seed(111)  

n_simulations <- 1000
private_coefficients <- replicate(
  n_simulations,
  releaseResponse(mechanism, privacyParams = privacy_params_sim, X = sub3)$response
)

# get non-private coefficients
non_private_coeffs <- coef(lm(lnrev ~ budget + popsqrt * genre + o_lang + vc, data = sub3))

# create a data frame for private coefficients
private_df <- as.data.frame(t(private_coefficients))
colnames(private_df) <- names(non_private_coeffs)
private_df$Simulation <- seq_len(n_simulations)

# reshape data for plotting
private_long <- private_df %>%
  pivot_longer(cols = -Simulation, names_to = "Coefficient", values_to = "Value")

# prepare non-private coefficients for plotting
non_private_df <- data.frame(
  Coefficient = names(non_private_coeffs),
  Value = as.numeric(non_private_coeffs)
)

# plot private vs. non-private coefficients
ggplot() +
  geom_density(data = private_long,
               aes(x = Value, fill = "Private"),
               alpha = 0.5) +
  geom_vline(data = non_private_df,
             aes(xintercept = Value,
                 linetype = "Non-Private"),
             color = "black", size = 0.5) +
  facet_wrap(~ Coefficient, scales = "free") +
  labs(title = "Distributions of Private vs. Non-Private Coefficients",
       x = "Coefficient Value", y = "Density",
       fill = "Type", linetype = "Type") +
  scale_fill_manual(values = c("Private" = "red")) +
  scale_linetype_manual(values = c("Non-Private" = "dashed")) +
  theme_minimal()

# validate means of private and non-private coefficients
cat("Mean of private budget coefficient:\n", mean(private_df$genre), "\n")
cat("Non-private budget coefficient:\n", non_private_coeffs["genre"], "\n")
```

The density plots showed that the private coefficients were distributed around 
their non-private counterparts, reflecting the noise introduced by the Laplace 
mechanism. For most coefficients, the distributions had a relatively narrow 
spread, indicating that the noise did not overly distort the regression results. 

# Conclusion

Our first hypothesis that different attributes of a feature movie significantly 
impacts its revenue has been proven correct by our final model. It was also
shown that a movie’s budget and genre contribute significantly, if not more, to
the movie’s revenue.

Our model is imperfect despite its high adjusted R2 value and significant
predictors. It failed the Shapiro Wilk test for normality and other assumption
checks. Using the entire data set instead of limiting it to specific genres and 
languages might have improved the approximation of normality. Time and knowledge
constraints also prevented us from conducting additional, more detailed analyses.
Additionally, the source of the voting data is unknown, which may introduce
bias.

With the above in mind, we propose the following future directions to improve
our model and expand on the studies:

1. Address assumption violations: Perform more transformations on variables or
use more data to achieve normality and constant variance of residuals. A few
transformations being considered are logarithmic, square root, and inverse
transformations.

2. Alternative modeling approaches: Consider generalized linear models (GLMs)
to accommodate non-normal error distributions or nonlinear relationships.

# Bibliography

Apple differential privacy technical overview. (n.d.).
<https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf>

Differential privacy and the 2020 census. (2021).
<https://www.census.gov/content/dam/Census/library/factsheets/2021/differential-privacy-and-the-2020-census.pdf>

Joseph Ficek, Wei Wang, Henian Chen, Getachew Dagne, Ellen Daley,
Differential privacy in health research: A scoping review, *Journal of
the American Medical Informatics Association*, Volume 28, Issue 10,
October 2021, Pages 2269–2276, <https://doi.org/10.1093/jamia/ocab135>

Practical Differential Privacy: Healthcare Scenario. (2024). Pitt.edu.
<https://www.sis.pitt.edu/lersais/research/sahi/resources/labs/dp/Lab_differential_privacy.pdf>

Rubinstein, B., & Aldà, F. (2017). diffpriv: An R package for easy
differential privacy. *Journal of Machine Learning Research, 18, 1–5.*
<https://cran.r-project.org/web/packages/diffpriv/vignettes/diffpriv.pdf>
