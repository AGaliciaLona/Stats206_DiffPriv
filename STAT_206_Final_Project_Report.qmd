---
title: "STAT 206 Final Project Report"
author: "Axel Galicia Lona, Linlin Liu, and Nancy Lopez"
date: "2024-12-13"
echo: true
format:
  html:
    toc: true
    toc-location: left
    embed-resources: true
    code-line-numbers: true
  pdf:
    code-line-numbers: true
editor: 
  markdown: 
    wrap: 72
---

\newpage
\tableofcontents
\newpage
\listoffigures
\newpage
\listoftables
\newpage

# Background and Motivation

Movies are a visual art form that simulates and communicates thoughts,
actions, ideas, and realities by combining camera recordings with sound.
With rapid population expansion and technical improvement, the movie
industry grows increasingly sophisticated. The industry's maturity has
also boosted the rate at which movies are made and produced, and every
year, a new batch of films is released in theaters. Therefore, studios,
producers, and investors need reliable predictions to make informed
decisions about budgeting, marketing, and resource allocation.

As viewers, we are also interested in the factors that contribute to a
film's revenue, because these generally define its success. Furthermore,
the highly competitive nature of the film industry, as well as the huge
economic impact of movie income, motivated us to pursue this project.
With abundant data available on various aspects of movies, including
budget, cast, genre, release dates, and social media buzz, applying
statistical models becomes a valuable tool for analysis. Thus, we chose
to build a multilinear regression model for our project.

Differential privacy (DP) is essential for datasets containing sensitive
aspects of movies, such as budget, revenue, cast, and others, as it
ensures that individual data points cannot be traced back to their
sources. For instance, protecting details like a movie's budget prevents
competitors from exploiting financial strategies, while preserving
anonymity in user-related data safeguards privacy rights. By adding
controlled random noise to the data, differential privacy balances
utility with confidentiality, enabling meaningful analysis without
exposing proprietary or personal information.

# Introduction To Differential Privacy

DP is a statistical framework designed to protect individuals' sensitive
information in a dataset. It protects privacy by adding random noise to
the outputs of data analyses before they are shared with any third-party
such as scientists, companies, or other analysts. This ensures that no
single individual's data can be identified or inferred, even if someone
has additional knowledge about the dataset. The goal of DP is to balance
privacy protection with the need for accurate and meaningful data
analysis, allowing organizations to extract valuable insights without
compromising individual privacy.

The formal definition of DP states that for any 2 datasets, D and D'
differing by one individual data point, the probability of mechanism $M$
producing a certain result should be very close for both datasets. This
can be mathematically expressed as:
$Pr(M(D)\in R)\leq exp(\epsilon)*Pr(M(D') \in R)$ where $\epsilon$ is
the privacy parameter. A small privacy parameter means strong privacy
since the output reveals less information about individual data. By
having two similar datasets with one missing a data point, it becomes
extremely difficult to guess whether a specific persons data was used,
hence protecting individuals' privacy.

A good noise-adding mechanism should privatize data while maintaining
its usefulness, enabling meaningful analysis without risking
individualsâ€™ sensitive information. This method is particularly
important in today's world, where vast amounts of data are constantly
being collected. From our phones and social media platforms to streaming
services and other technologies, our data is tracked to provide better
algorithms and more personalized recommendations. DP plays a crucial
role in protecting our privacy in fields like healthcare, technology,
and beyond.

# Differential Privacy Applications

DP introduces mathematical noise to the data, ensuring that insights can
be derived without revealing specific details about any individual
involved. This balance between utility and privacy has made DP an
essential tool in modern data-driven industries.

Tech companies like Apple are pioneers in integrating DP into
their services. For example, Apple uses DP in features like QuickType (
Differential Privacy a Privacy-Preserving System, n.d.), where it analyzes
typing patterns to improve text prediction and autocorrect functionality.
This ensures no sensitive typing data is accessible to the company while still enabling advanced machine learning models to learn user behaviors. Similarly, differential privacy underpins Apple's emojis and media usage analytics,
which help them refine features across devices without linking data to
individual users.

Governments, including the U.S. Census Bureau, rely on DP as well to
protect participants' confidentiality. In the 2020 Census, DP was
introduced to anonymize demographic data shared with policymakers,
researchers, and the public. By adding noise to reported statistics, the
system prevents attackers from deducing private details, even if
datasets are combined with other publicly available information. This
ensures that privacy is upheld while still providing accurate and
actionable population trends for resource allocation, urban planning,
and legislative redistricting.

Another field that widely employs DP is the healthcare sector. Hospitals
and research institutions can share anonymized patient data for
large-scale studies, like predicting disease outbreaks or evaluating
treatment efficacy. DP ensures that even if the dataset is intercepted,
it is mathematically improbable for attackers to extract identifiable
patient information.

Lastly, DP is increasingly used in online education platforms and remote
work tools to safeguard user data. For instance, platforms like Coursera
and Duolingo might use DP to analyze user progress and learning habits
while protecting individual study behaviors. Tools like Zoom or
Microsoft Teams could use DP in aggregating call analytics, such as
meeting duration or user engagement trends, while ensuring participant
anonymity.

By enabling secure data analysis, DP empowers industries to innovate
without eroding public trust. This is especially critical in a world
increasingly reliant on data for decision-making, where user privacy is
both a legal mandate and an ethical responsibility.

# Noise Implementation Methods

The three most common noise implementation methods for Differential
Privacy are Gaussian, Laplace, and Exponential. For this project, we
used the Laplace mechanism, which provides $(\epsilon, 0)$-differential
privacy by adding noise sampled from a Laplace distribution to query
results. The noise is drawn as
$X \sim \text{Laplace}(0, \Delta f / \epsilon)$, where $0$ is the mean
of the distribution, $\Delta f$ is the sensitivity of the query, and
$\epsilon$ is the privacy budget. The sensitivity $\Delta f$ represents
the maximum change in a query result caused by the addition or removal
of a single data point and is proportional to the $L_1$ sensitivity of
the query, which measures this maximum change using the sum of absolute
differences.

The Laplace mechanism has two key implications: (1) the added noise is
proportional to the $L_1$ sensitivity of the query, and (2) the noise
magnitude is inversely related to the privacy budget $\epsilon$. A
smaller $\epsilon$ value results in stronger privacy by adding more
noise, making it harder to infer individual data points, whereas a
larger $\epsilon$ value reduces noise, improving accuracy at the cost of
weaker privacy guarantees. Sensitivity $\Delta f$ depends on the dataset
and query, while $\epsilon$ is independent of both and serves as a
user-defined parameter to balance privacy and utility. The Laplace
mechanism is particularly effective for numerical queries with low or
intermediate $L_1$ sensitivity but is less suited for high-sensitivity
queries, where large noise may reduce data utility. Additionally, it
does not apply to non-numerical queries.

The Gaussian mechanism is another commonly used method for implementing
differential privacy, providing $(\epsilon, \delta)$-differential
privacy. Unlike the Laplace mechanism, which adds noise from a Laplace
distribution, the Gaussian mechanism adds noise sampled from a Gaussian
(normal) distribution. The noise is drawn as
$X \sim \mathcal{N}(0, \sigma^2)$, where $0$ is the mean of the
distribution, and $\sigma$ is the standard deviation of the noise. The
standard deviation $\sigma$ is determined by the query's sensitivity
$\Delta f$, the privacy budget $\epsilon$, and a small relaxation
parameter $\delta$ according to the formula
$\sigma = \frac{\Delta f \cdot \sqrt{2 \ln(1.25/\delta)}}{\epsilon}$.

The sensitivity $\Delta f$ represents the maximum change in the query
result caused by the addition or removal of a single data point and can
be measured using either $L_1$ or $L_2$ sensitivity. The Gaussian
mechanism is particularly well-suited for numerical queries where $L_2$
sensitivity, calculated using the Euclidean norm, is smaller than $L_1$
sensitivity, allowing for less noise to be added.

The key difference between the Gaussian and Laplace mechanisms lies in
their privacy guarantees: the Gaussian mechanism provides
$(\epsilon, \delta)$-differential privacy, which allows a small
probability $\delta$ of relaxing the strict privacy requirement, making
it useful for high-sensitivity queries or larger datasets. However, this
relaxation means it does not offer the same strict guarantees as the
Laplace mechanism. The Gaussian mechanism is most effective for queries
with moderate sensitivity and when some flexibility in privacy
guarantees is acceptable.

The Exponential mechanism is a versatile method for implementing
differential privacy, suitable for both numerical and categorical
queries. Unlike the Laplace and Gaussian mechanisms, which add noise
directly to the query results, the Exponential mechanism introduces
randomness by selecting an output based on a probability distribution
weighted by a utility function. For a dataset $D$ and a possible output
$r$, the probability of selecting $r$ is proportional to
$\exp\left(\frac{\epsilon \cdot u(D, r)}{2 \Delta u}\right)$, where
$u(D, r)$ is the utility function that assigns a score to $r$ based on
its quality, and $\Delta u$ is the sensitivity of the utility function.

The sensitivity $\Delta u$ represents the maximum change in the utility
function caused by adding or removing a single data point in the
dataset. The privacy budget $\epsilon$ controls the balance between
privacy and accuracy: higher $\epsilon$ values favor outputs with higher
utility scores, while lower $\epsilon$ values increase randomness,
improving privacy at the expense of accuracy.

The Exponential mechanism is particularly useful when the query output
is not numeric or when selecting the best result from a set of
possibilities, such as recommending items or determining optimal
parameters. It ensures that outputs with higher utility scores are more
likely to be chosen while preserving privacy. However, defining an
appropriate utility function can be complex, and the mechanism may not
perform well for queries with high sensitivity.

# diffpriv() Package

We implemented DP to the movies dataest through an R package called
*diffpriv*. This package provides tools that support differential
privacy, making it suitable for releasing analyses on sensitive data to
third parties. *diffpriv* implements genetic mechanisms and a
sensitivity sampler that replaces the exact sensitivity bounds with
empirical estimates. This is convenient since calculating the exact
sensitivity can be very complex and computationally expensive.

DP noise is usually added to the results of a query, based on the
sensitivity of the query. When a query has high sensibility, the data
can be easily affected by one individual data point, therefore high
sensitivity implies the application of more noise. Less sensitivity, on
the other hand, needs less noise.

When using *diffpriv*, instead of calculating the worst-case
sensitivity, we have the sensitivity sampler that estimates the
sensitivity based on the sample data given. This is very useful because
one does not have to do all complicated calculations in order to obtain
the sensitivity, making the process much more time efficient. It also is
very practical since when working with real data, it obtains a good
estimate for most queries, even for large datasets. Lastly, using this
method can add more precise data, leading to more accurate results while
protecting privacy.

Another great benefit of using this package is that itâ€™s not very
complicated to use. The first step is to set a target function, which
means one has to set what is to be privatized. This could be a dataset,
a mean value, or, in the case of this project, linear regression
coefficients. Next, we determine the sensitivity manually by using the
Laplace distribution. The global sensitivity is$\frac{1}{n}$, where n is
the number of observations in our final MLR model. This is because, for
queries like mean or regression coefficients, the impact of one data
point is smaller as the dataset grows. So, with larger datasets, the
noise needed to protect privacy is reduced, as the influence of any
single data point becomes less significant. This makes the process more
efficient while still ensuring privacy.

The next step is to determine the privacy budget $\epsilon$, which
controls the trade-off between privacy and accuracy. A high privacy
budget means less noise, resulting in less privacy being protected, and
a low privacy budget means more noise and thus more privacy is added to
the dataset. In our case, we chose a privacy budget of 1, which balances
accuracy and privacy in a moderate way. After establishing our privacy
parameters, which determine how much noise to add, we apply the
*DPMechLaplace()* function. This function adds Laplace noise to the
query results. Once the noise is applied, we release the privatized
output using the *releaseResponse()* function. This shows how the data
looks with the noise. This is the final result to be providedi to a
third-party, ensuring that the privacy of individuals is protected.

# Implementation of DiffPriv

-   now for our example of using diffpriv with linear model.

# Bibliography

Differential Privacy A privacy-preserving system. (n.d.). [https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf](https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf)

Practical Differential Privacy: Healthcare Scenario. (2024). Pitt.edu. [https://www.sis.pitt.edu/lersais/research/sahi/resources/labs/dp/Lab_differential_privacy.pdf](https://www.sis.pitt.edu/lersais/research/sahi/resources/labs/dp/Lab_differential_privacy.pdf)

Rubinstein, B., & AldÃ , F. (2017). diffpriv: An R Package for Easy Differential Privacy. *Journal of Machine Learning Research, 18, 1â€“5.* [https://cran.r-project.org/web/packages/diffpriv/vignettes/diffpriv.pdf](https://cran.r-project.org/web/packages/diffpriv/vignettes/diffpriv.pdf)