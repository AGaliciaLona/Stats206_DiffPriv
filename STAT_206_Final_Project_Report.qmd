---
title: "STAT 206 Final Project Report"
author: "Axel Galicia Lona, Linlin Liu, and Nancy Lopez"
date: "2024-12-13"
echo: true
format:
  html:
    toc: true
    toc-location: left
    embed-resources: true
    code-line-numbers: true
  pdf:
    code-line-numbers: true
editor: 
  markdown: 
    wrap: 72
---

# Background and Motivation

With rapid population expansion and technical improvement, the movie
industry grows increasingly sophisticated nowadays. Therefore, studios,
producers, and investors need reliable predictions to make informed
decisions about budgeting, marketing, and resource allocation.

As viewers, we are also interested in the factors that contribute to a
film's revenue, because these generally define its success. Furthermore,
the highly competitive nature of the film industry, as well as the huge
economic impact of movie income, motivated us to pursue this project.
Thus, we chose to build a multiple linear regression (MLR) model for our
project.

Differential privacy (DP) is essential for datasets containing sensitive
aspects of movies, as it ensures that individual data points cannot be
traced back to their sources. By adding controlled random noise to the
data, differential privacy balances utility with confidentiality,
enabling meaningful analysis without exposing proprietary or personal
information and making it a crucial tool in this data-driven
exploration.

# Hypotheses

For this project, we propose the following hypotheses:

1.  Different attributes of a movie significantly impacts its revenue.

2.  A movie’s budget and genre contribute more significantly than
    others.

3.  The private and non-private coefficients obtained for DP will be
    similar.

# Final MLR Model

Our proposed final MLR model is:

$$Y = 2.6366e^{-5}X_1 + 8.5311e^{-1}X_2 + 3.3198X_3 - 7.7753e^{-1}X_4 +
1.3633e^{-4}X_5 + 5.5155X_6 + \epsilon$$

where

-   $Y$ = *lnrev*

-   $X_1$ = *budget*

-   $X_2$ = *popsqrt*

-   $X_3$ = *genres*

-   $X_4$ = *popsqrt* \* *genre*

-   $X_5$ = *vote count*

-   $X_6$ = *original language*

The assumptions of $\epsilon$ are as follows:

-   linearity: a linear relationship exists between the response and
    predictor variables

-   constant variance: the variability about the least squares line is
    generally constant

    -   Var($\epsilon_i$) = $\sigma^2$, where $i$ = 1, 2, ..., n

-   normality: the distribution of the residuals is approximately normal

-   independence: the residuals are independent from each other

    -   for $\epsilon_i$ and $\epsilon_j$, $i \neq j$, where $i$ = 1, 2,
        ..., n, and $j$ = 1, 2, ..., n

In order to better observe specific predictors, we first converted
categorical predictor variables into binary integers, assigning 1 to
"Drama" and "en" and 0 to "Comedy" and "fr."

We utilized scatter plots to examine the relationship between each
predictor and the response. We observed distinct patterns in *runtime*,
*vote_average*, and *vote_count*. The scatter plots for *budget* and
*popularity* did not reveal a clear linear relationship or direction.

# Introduction To Differential Privacy

DP is a statistical method that adds random noise to analysis results to
protect individual privacy in datasets. It ensures that no individual’s
data can be identified or inferred, even with extra knowledge, allowing
for accurate insightswithout compromising privacy.

The formal definition of DP states that for 2 datasets, D and D′,
differing by one individual, the probability of mechanism $M$ producing
a result should be nearly identical for both. This is mathematically
expressed as: $Pr(M(D)\in R)\leq exp(\epsilon)*Pr(M(D') \in R)$ where
$\epsilon$ is the privacy parameter. A smaller $\epsilon$ ensures
stronger privacy, as it reveals less about individual data. With two
similar datasets differing by one data point, it becomes difficult to
determine if a specific person's data was used, thus protecting privacy.

Formally, DP guarantees that for any two datasets, D and D', differing
by one data point, the probability of a mechanism $M$ producing a
specific output remains similar. Mathematically:
$Pr(M(D)\in R)\leq exp(\epsilon) \cdot Pr(M(D') \in R)$, where
$\epsilon$ is the privacy parameter, with smaller values indicating
stronger privacy. This ensures that whether or not a specific
individual's data is included, the outputs are nearly indistinguishable,
protecting privacy effectively.

An effective noise-adding mechanism preserves privacy while maintaining
data utility, enabling analysis without compromising sensitive
information. In today's data-driven world, DP is crucial for balancing
privacy and technological progress.

# Differential Privacy Applications

DP introduces mathematical noise to the data, ensuring that insights can
be derived without revealing specific details about any individual
involved. This balance between utility and privacy has made DP an
essential tool in modern data-driven industries.

Tech companies like Apple are pioneers in integrating DP into their
services. For example, Apple uses DP in features like QuickType ( Apple
differential privacy technical overview, n.d.), where it analyzes typing
patterns to improve text prediction and autocorrect functionality. This
ensures no sensitive typing data is accessible to the company while
still enabling advanced machine learning models to learn user behaviors.

Governments, including the U.S. Census Bureau, rely on DP as well to
protect participants' confidentiality. In the 2020 Census, DP was
introduced to anonymize demographic data shared with policymakers,
researchers, and the public (Differential privacy and the 2020 census,
2021). Adding noise to the reported statistics ensures that privacy is
upheld while still providing accurate and actionable population trends
for resource allocation, urban planning, and legislative redistricting.

Another field that widely employs DP is the healthcare sector. Hospitals
and research institutions can share anonymized patient data for
large-scale studies, like predicting disease outbreaks or evaluating
treatment efficacy. DP ensures that even if the dataset is intercepted,
it is mathematically improbable for attackers to extract identifiable
patient information.

By enabling secure data analysis, DP empowers industries to innovate
without eroding public trust. This is especially critical in a world
increasingly reliant on data for decision-making, where user privacy is
both a legal mandate and an ethical responsibility.

# Noise Implementation Methods

The three primary noise mechanisms for DP are Gaussian, Laplace, and
Exponential. This project uses the Laplace mechanism, which provides
$(\epsilon, 0)$-DP by adding noise from a Laplace distribution:
$X \sim \text{Laplace}(0, \Delta f / \epsilon)$. Here, $\Delta f$ is the
query sensitivity, representing the maximum change caused by adding or
removing a single data point, and $\epsilon$ is the privacy budget. A
smaller $\epsilon$ increases noise for stronger privacy, while a larger
$\epsilon$ reduces noise for better accuracy. The Laplace mechanism
works best for numerical queries with low to moderate $L_1$ sensitivity
but is unsuitable for high-sensitivity or non-numerical queries.

The Gaussian mechanism, offering $(\epsilon, \delta)$-DP, adds noise
sampled from a Gaussian distribution: $X \sim N(0, \sigma^2)$. The noise
standard deviation $\sigma$ is determined by $\Delta f$, $\epsilon$, and
$\delta$, a relaxation parameter, using
$\sigma = \frac{\Delta f \cdot \sqrt{2 \ln(1.25/\delta)}}{\epsilon}$.
Gaussian noise is particularly effective for queries where $L_2$
sensitivity is smaller than $L_1$ sensitivity, as it introduces less
noise. While the mechanism is suitable for high-sensitivity or large
datasets, its privacy guarantees are slightly weaker than the Laplace
mechanism.

The Exponential mechanism applies to numerical and categorical queries
by selecting an output based on a probability distribution weighted by a
utility function. The probability of selecting output $r$ is
proportional to
$\exp\left(\frac{\epsilon \cdot u(D, r)}{2 \Delta u}\right)$, where
$u(D, r)$ is the utility function and $\Delta u$ is its sensitivity. It
is ideal for scenarios requiring non-numeric outputs, such as ranking or
parameter selection. However, it relies heavily on defining a suitable
utility function and may perform poorly for high-sensitivity queries.

# diffpriv() Package

We implemented DP to the final MLR model we built using the R package
*diffpriv()*. This package simplifies DP implementation by providing
tools like genetic mechanisms and a sensitivity sampler, which estimates
sensitivity empirically instead of calculating exact bounds. This
approach is computationally efficient, especially for complex queries or
large datasets.

DP noise is added to query results based on their sensitivity. Using
*diffpriv()*, the sensitivity sampler estimates sensitivity directly
from sample data, bypassing complex calculations and delivering
practical and accurate results for most queries.

To use the package, the process begins by defining the target function
for privatization, such as a dataset, mean, or regression coefficients.
For our project, we privatized linear regression coefficients.
Sensitivity was determined using the Laplace distribution, with global
sensitivity calculated as $\frac{1}{n}$, where $n$ is the number of
observations in the MLR model. Larger datasets reduce the impact of
individual data points, requiring less noise and improving efficiency
while maintaining privacy.

The privacy budget $\epsilon$, which balances privacy and accuracy, was
set to 1, offering moderate privacy. After defining privacy parameters,
the *DPMechLaplace()* function added Laplace noise to query results, and
the *releaseResponse()* function released the privatized output. This
final output protects individual privacy while ensuring the data's
utility for third-party analysis.

### background code

available in qmd
```{r, include=FALSE}
library(tidyverse)
original <- read.csv(file.choose(), header = T)
# subset data
sub1 <- subset(original, select = c("genres", "original_language",
                                    "popularity","budget","revenue",
                                    "runtime","vote_average", "vote_count"))

# replace all empty cells and 0's with NA
sub1[sub1 == "" | sub1 == 0] <- NA

# clean the data with selected restrictions
sub2 <- sub1 %>%
  na.omit() %>%
  filter(revenue >= 10000) %>%
  filter(budget >= 10000) %>%
  filter(runtime >= 40) %>%
  filter(vote_count >= 95) %>%
  filter(vote_average > 0) 

# split genres and find the top 2 genres
sub2$genres <- sapply(strsplit(sub2$genres, "-"), `[`, 1)
genre_freq <- table(sub2$genres)
genre_rank <- sort(genre_freq, decreasing = TRUE)
head(genre_rank, 2)

# find the top 2 original languages
lang_freq <- table(sub2$original_language)
lang_rank <- sort(lang_freq, decreasing = T)
lang_rank[1:2]

# subset again using only the top 2 genres and the top 2 original languages
# this is the final data set to be used for model building
sub3 <- sub2 %>%
  filter(genres == "Drama" | genres == "Comedy") %>%
  filter(original_language == "en" | original_language == "fr")

# divide revenue and budget by 1000 for easier calculations and visualization
sub3$budget <- as.numeric(as.character(sub3$budget)) / 1000
sub3$revenue <- as.numeric(as.character(sub3$revenue)) / 1000

rev <- sub3$revenue
vc <- sub3$vote_count
budget <- sub3$budget
pop <- sub3$popularity
va <- sub3$vote_average
rt <- sub3$runtime

# create dummy variables for each cat var
genre <- ifelse(sub3$genres == "Drama", 1, 0)
o_lang <- ifelse(sub3$original_language == "en", 1, 0)
lnrev <- log(sub3$revenue)
popsqrt <- sqrt(pop)

```



# Implementation of diffpriv()

The Laplace mechanism, a widely used approach for DP, was employed to
protect the outputs of our final MLR model, which estimated the
relationship between log revenue (*lnrev*) and predictors including
*budget*, *popsqrt* \* *genre*, *o_lang*, and *vc*. The model's
coefficients, including the intercept, formed the query output to be
privatized. Adding Laplace noise to these coefficients ensured that
individual data points remained secure.

The sensitivity $\Delta f$ and the privacy budget $\epsilon$ were set to
$1$. This balanced privacy and accuracy, with smaller $\epsilon$ values
increasing noise for stronger privacy and larger values reducing noise
for better accuracy.

The query output dimension was $7$, encompassing one intercept and six
predictors. Noise was applied to each coefficient, preserving privacy
without compromising the overall utility of the model.

```{r}
library(diffpriv)

set.seed(111)  # for reproducibility

# define target function for linear regression coefficients with intercept
target_function <- function(X) {
  model_sim <- lm(lnrev ~ budget + popsqrt * genre + o_lang + vc, data = X)
  return(coef(model_sim))  # returns coefficients including the intercept
}

# define the Laplace mechanism
# the sample size is 2624
# dims = 7 because there's 1 intercept and 6 variables
mechanism <- DPMechLaplace(target = target_function, sensitivity = 1 / 2624,
                           dims = 7)

# set privacy parameters
privacy_params_sim <- DPParamsEps(epsilon = 1)

# generate private response for one run
private_response <- releaseResponse(mechanism,
                                    privacyParams = privacy_params_sim,
                                    X = sub3)

# display non-private and private coefficients for comparison
cat("Non-private coefficients:\n", coef(lm(lnrev ~ budget + popsqrt * genre + o_lang + vc, data = sub3)), "\n")
cat("Private coefficients:\n", private_response$response, "\n")
```

We then conducted a simulation study involving 1,000 iterations to
evaluate the impact of the Laplace mechanism on the coefficients. The
objective was to assess how the mechanism's added noise affects the
coefficients' distribution and compare the results to the non-private
regression coefficients.

## Simulations

In each iteration, the Laplace mechanism was applied to the regression
coefficients, adding noise proportional to the sensitivity of the query
and inversely proportional to the privacy budget ($\epsilon$). Using
*releaseResponse()*, we generated a set of private coefficients for each
simulation. These private coefficients were stored for further analysis.

A standard MLR model was fitted to the dataset to compute the
non-private coefficients. These served as a baseline for comparison and
were calculated once since they remain constant across simulations.

## Visualizations

Density plots were created to display the distributions of the private
coefficients across the 1,000 simulations. Vertical lines were added to
indicate the corresponding non-private coefficients, providing a
comparison of their centrality and variability. Each coefficient was
plotted in a separate facet for clarity.

```{r,warning=FALSE}
set.seed(111)  

n_simulations <- 1000
private_coefficients <- replicate(
  n_simulations,
  releaseResponse(mechanism, privacyParams = privacy_params_sim, X = sub3)$response
)

# get non-private coefficients
non_private_coeffs <- coef(lm(lnrev ~ budget + popsqrt * genre + o_lang + vc, data = sub3))

# create a data frame for private coefficients
private_df <- as.data.frame(t(private_coefficients))
colnames(private_df) <- names(non_private_coeffs)
private_df$Simulation <- seq_len(n_simulations)

# reshape data for plotting
private_long <- private_df %>%
  pivot_longer(cols = -Simulation, names_to = "Coefficient", values_to = "Value")

# prepare non-private coefficients for plotting
non_private_df <- data.frame(
  Coefficient = names(non_private_coeffs),
  Value = as.numeric(non_private_coeffs)
)

# plot private vs. non-private coefficients
ggplot() +
  geom_density(data = private_long,
               aes(x = Value, fill = "Private"),
               alpha = 0.5) +
  geom_vline(data = non_private_df,
             aes(xintercept = Value,
                 linetype = "Non-Private"),
             color = "black", size = 0.5) +
  facet_wrap(~ Coefficient, scales = "free") +
  labs(title = "Distributions of Private vs. Non-Private Coefficients",
       x = "Coefficient Value", y = "Density",
       fill = "Type", linetype = "Type") +
  scale_fill_manual(values = c("Private" = "red")) +
  scale_linetype_manual(values = c("Non-Private" = "dashed")) +
  theme_minimal()

# validate means of private and non-private coefficients
cat("Mean of private budget coefficient:\n", mean(private_df$genre), "\n")
cat("Non-private budget coefficient:\n", non_private_coeffs["genre"], "\n")
```

The density plots showed that the private coefficients were distributed
around their non-private counterparts, reflecting the noise introduced
by the Laplace mechanism. For most coefficients, the distributions had a
relatively narrow spread, indicating that the noise did not overly
distort the regression results.

# Conclusion

## MLR

The results of our final MLR model confirm that different attributes of
a movie significantly impact its revenue, with budget and genre being
particularly significant predictors.

However, our model, despite having a high adjusted $R^{2}$ value and
significant predictors, failed the Shapiro-Wilk test for normality and
other assumption checks. Expanding the dataset to include all genres and
languages might improve the approximation of normality, and the unknown
source of the voting data could introduce bias.

With the above in mind, we propose the following future directions to
improve our model and expand on the studies:

1.  Perform more transformations on variables or use more data to
    achieve normality and constant variance of residuals. A few
    transformations being considered are square root and inverse
    transformations.

2.  Consider generalized linear models (GLMs) to accommodate non-normal
    error distributions or nonlinear relationships.

## DP

The private and non-private coefficients obtained using the *diffpriv()*
package are very close, indicating that the added noise has minimally
impacted the statistical estimates. This is likely due to the robustness
of the statistical model, as models that are less sensitive to small
perturbations or outliers can naturally produce similar private and
non-private coefficients even after noise is added.

In this project, having similar private and non-private coefficients is
highly desirable because maintaining high utility and preserving much of
the accuracy of the original results is a priority.

The density plots showed that the private coefficients were distributed
around their non-private counterparts, suggesting that the noise added
for DP was centered and unbiased. This outcome indicates a
well-calibrated privacy mechanism that effectively balances privacy and
utility, minimizing the risk of overestimating or underestimating the
coefficients and supporting reliable inferences.

# Bibliography

Apple differential privacy technical overview. (n.d.).
<https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf>

Differential privacy and the 2020 census. (2021).
<https://www.census.gov/content/dam/Census/library/factsheets/2021/differential-privacy-and-the-2020-census.pdf>

Joseph Ficek, Wei Wang, Henian Chen, Getachew Dagne, Ellen Daley,
Differential privacy in health research: A scoping review, *Journal of
the American Medical Informatics Association*, Volume 28, Issue 10,
October 2021, Pages 2269–2276, <https://doi.org/10.1093/jamia/ocab135>

Practical Differential Privacy: Healthcare Scenario. (2024). Pitt.edu.
<https://www.sis.pitt.edu/lersais/research/sahi/resources/labs/dp/Lab_differential_privacy.pdf>

Rubinstein, B., & Aldà, F. (2017). diffpriv: An R package for easy
differential privacy. *Journal of Machine Learning Research, 18, 1–5.*
<https://cran.r-project.org/web/packages/diffpriv/vignettes/diffpriv.pdf>




