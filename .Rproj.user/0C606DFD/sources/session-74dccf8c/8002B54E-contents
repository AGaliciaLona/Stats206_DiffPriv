---
title: "STAT 206 Final Project MLR"
author: "Axel Galicia Lona, Linlin Liu, amd Nancy Lopez"
date: "2024-12-13"
echo: true
format:
  html:
    toc: true
    toc-location: left
    embed-resources: true
    code-line-numbers: true
  pdf:
    code-line-numbers: true
---

\newpage
\tableofcontents
\newpage
\listoffigures
\newpage
\listoftables
\newpage

# Introduction

## Background and Motivation

Movies are a visual art form that simulates and communicates thoughts, actions,
ideas, and realities by combining camera recordings with sound. With rapid
population expansion and technical improvement, the movie industry grows
increasingly sophisticated. The industry's maturity has also boosted the rate
at which movies are made and produced, and every year, a new batch of films is  released in theaters. Therefore, studios, producers, and investors need
reliable predictions to make informed decisions about budgeting, marketing,
and resource allocation.

As viewers, we are also interested in the factors that contribute to a film's
revenue, because these generally define its success. Furthermore, the highly competitive nature of the film industry, as well as the huge economic impact of
movie income, motivated us to pursue this project. With abundant data available
on various aspects of movies, including budget, cast, genre, release dates, and
social media buzz, applying statistical models becomes a valuable tool for
analysis. Thus, we chose to build a multilinear regression model for our
project.

Differential privacy is crucial for datasets containing sensitive aspects of
movies, such as budget, revenue, cast, and others, as it ensures that
individual data points cannot be traced back to their sources. For instance, protecting details like a movie's budget prevents competitors from exploiting financial strategies, while preserving anonymity in user-related data
safeguards privacy rights. By adding controlled random noise to the data,
differential privacy balances utility with confidentiality, enabling meaningful analysis without exposing proprietary or personal information.

## Objectives

This project aims to deepen the understanding of how different factors may
impact a movie’s revenue and to what degree they will influence the box office.
In other words, we aim to determine whether the data provides sufficient
evidence to indicate that these variables contribute information for the
prediction of a movie’s revenue. If so, we seek to identify which variables
contribute the most to accurate predictions. Moreover, we would like to explore
the possibility of protecting sensitive information in the data set by adding a controlled amount of random noise using differential privacy.

These will be achieved by analyzing the data set *movies.csv*, which samples
various components of a movie, including genre, release date, budget,
production company, and cast, from 722,463 movies.

## Hypotheses

Based on the information provided by the data set, we propose the following hypotheses:

1. Different attributes of a feature movie significantly impacts its revenue.

2. A movie’s budget and genre contribute more significantly than others.

# Data Cleaning

Having an original data set of 722,463 observations and 20 variables would make
it rather difficult for us to build a model and increase the model complexity,
so we first decided to eliminate 12 variables as the first step of our data
cleaning process.

Next, we further cleaned the data set by completing the following:

* Omitted all NA's and 0's

* Chose movies with revenue and budget greater than or equal to $10,000

* Chose movies with run time greater than or equal to 40 minutes

* Chose movies with vote count greater than or equal to 95

* Chose movies with vote average greater than 0

The above left us with a data set of 6,804 observations and 8 variables.
However, due to the concern that having too many categories in our chosen
categorical variables, we decided to chose the top two categories for each. We
also divided all variables with unites in dollars by 1,000 for to facilitate
analysis. At the end, we obtained a total of 2,624 observations and 8 variables
as the final data set.

**NOTE:** For the code, please refer to the "Data Cleaning" section in the
Appendix.

# Data Description

For the model building process, we will now be working with the following
variables:

- *genres*: genre of the movie

- *original language*: the language in which the movie was first released

- *popularity*: the amount of times a movie’s page has been visited according
to TMDB metric

- *budget*: the total projected costs of making a movie

- *runtime*: duration of the movie

- *vote average*: average of votes given by TMDB users

- *vote count*: total number of TMDB votes a movie received

- *revenue*: the amount of money a movie raised by ticket sales, output target

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
Variable & Unit & Type & Min & Max & SD\\
\hline 
genres & Drama or Comedy & cat predictor & - & - & -\\
\hline 
original language & en or fr & cat predictor & - & - & -\\
\hline
popularity & - & num cont predictor & 0.82 & 3,994.34 & 86.44\\
\hline 
budget & thousand dollars & num cont predictor & 12.00 & 20,000.00 & 23,242.17\\
\hline
runtime & minutes & num cont predictor & 62 & 254 & 20.05\\
\hline
vote average & - & num disc predictor & 3.30 & 8.71 & 0.76\\
\hline
vote count & - & num disc predictor & 95 & 31,145 & 2,543.94\\
\hline
revenue & thousand dollars & num cont response & 10.16 & 2,187,464.00 & 101,669.80\\
\hline
\end{tabular}
\end{center}
\caption{Data Summary Statistics}
\end{table}

**NOTE:** The above variables are the ones found in the original data set. If necessary, we will transform or eliminate variables during model building to
facilitate the process. For more details, please refer to the "Model Building
and Diagnostics" section in the Appendix.

# Exploratory Data Analysis

The histogram (Fig. 1) indicates that the response variable *revenue* is not
normally distributed.

None of the numerical predictor is normally distributed except *vote_average*
(Fig. 2). There are almost an equal amount of Drama and Comedy movies, and
there are much more movies made in Englishn than in French. (Fig. 2)

The scatter plots of *budget*, *runtime*, and *vote_average* show patterns,
while the plots of *popularity* and *vote_count* do not, suggesting the first
three impact y but not the latter two (Fig. 3). Box plots of the two
categorical variables (Fig. 3) reveal numerous outliers within the data set.
The impact of these observations on our model-building process is currently
unknown. The medians for each boxplot are approximately the same, indicating
these variables might not significantly affect *revenue*. However, if
necessary, we aim to transform *revenue* and other variables to facilitate
smoother model-building and diagnostic steps.

We evaluate the degree of multicollinearity between pairs of predictor
variables in the model by examining their correlation coefficients. The
correlation coefficient ranges from -1 to 1, with a closer proximity to -1 or 1 indicating a higher correlation between the two variables. This is a concern we
aim to mitigate, as increased multicollinearity weakens the model’s ability to identify statistical significance. Please refer to the pairwise comparisons
(Fig. 4) for all the correlation coefficients.

Since most of the correlation coefficients are relatively small, we can
conclude that majority of the predictor variables do not exhibit
multicollinearity. An intermediate correlation between *budget* and *revenue*
is expected, as this is one of the variables we wanted to take a closer look at
and one of our hypotheses.

# Regression Analysis

Our proposed final model is

$$Y = 2.6366e^{-5}X_1 + 8.5311e^{-1}X_2 + 3.3198X_3 - 7.7753e^{-1}X_4 +
1.3633e^{-4}X_5 + 5.5155X_6 + \epsilon$$

where

- $Y$ = *lnrev*

- $X_1$ = *budget*

- $X_2$ = *popsqrt*

- $X_3$ = *genres*

- $X_4$ = *popsqrt* * *genre*

- $X_5$ = *vote count*

- $X_6$ = *original language*

The assumptions of $\epsilon$ are as follows:

- linearity: a linear relationship exists between the response and predictor
variables

- constant variance: the variability about the least squares line is
generally constant
  - VAR($\epsilon_i$) = $\sigma^2$, where $i$ = 1, 2, ..., n
  
- normality: the distribution of the residuals is approximately normal

- independence: the residuals are independent from each other
  - for $\epsilon_i$ and $\epsilon_j$, $i \neq j$, where $i$ = 1, 2, ..., n,
  and $j$ = 1, 2, ..., n
  
In order to better observe specific predictors, we first converted categorical
predictor variables into binary integers. We assigned 1 to "Drama" and "en" and
0 to "Comedy" and "fr."

We utilized scatter plots to examine the relationship between each predictor
variable and the response variable. We observed distinct patterns in *runtime*,
*vote_average*, and *vote_count*. The scatter plots for *budget* and
*popularity* did not reveal a clear linear relationship or direction.

## Interactions

For this project, we decided to the explore potential interactions between 
*popularity* and *genres* for the following reasons:

1.  Different genres might influence a movie's popularity in unique ways. By
exploring interactions, we can uncover whether the effect of a particular genre
on popularity changes when combined with other variables.

2. Including interaction terms can help improve the predictive accuracy of the
model. If the effect of a movie's genre on its popularity depends on other an interaction term can capture this dependency and lead to more precise
predictions.

## Model Building Before y Transformation

We fitted two models using the original response variable *revenue*: Model 1
with an interaction between *popularity* and *genres* and Model 2 with
*budget*, but the results were not ideal. Neither model passed the normality
test, and the scatter plots for *budget* and *genres*, two variables that we
wanted to focus on in our hypothesis, appeared to have on impact on the y, we
decided to perform a logarithmic transformation on *revenue* to see if this
will make a difference.

## Model Building After y Transformation

### Model 1

We fitted Model 1 with an interaction between *popularity* and *genres*, with
an intercept of 0. This choice was based on the rationale that, when all
predictor variables are equal to 0, the response variable should also be 0.
Setting the intercept to be 0 also yields a better adjusted $R^2$ value.
However, when assessing the normality of Model 1 through the Shapiro-Wilk test,
the assumption was violated. Consequently, we designated the Model 1 residuals
as our most recent response variable and proceeded with additional
investigations.

We generated scatter plots for each individual predictor variable against the
Model 1 residuals. All variables now appear to impact *lnrev* except
*vote_average*. This means that our transformation has made a difference.
We will continue to monitor *vote_average* before deciding whether or not to
exclude it from the model.

### Model 2

Model 2 was constructed with *budget* and a 0 intercept. We reassessed the
normality assumptions for Model 2 residuals using Model 1 residuals as the new
y values, and the Shapiro-Wilk test still indicated a failure.

Similar to the results of Model 1, *vote_average* again exhibited no pattern, implying it does affect *lnrev*. Additionally, *runtime* now looks more
constant than before. 

### Model 3

Next, we used the Model 2 residual as our y for Model 3, which incorporated *vote_count* with a 0 intercept. The residuals of the new model still failed
the Shapiro-Wilk test, and the histogram appears to be left-skewed.

We will exclude *vote_average* and *runtime*, with the latter collapsing more,
as these two look constant compared to all other variables.

Before continuing further analysis, we have decided to transform *popularity*
with a square root transformation for easier computation and model fitting and
hoping this will help us pass the normality assumption check.

### Model 4

In this model, we fitted the interaction between *popsqrt* and *genres*, with
an intercept of 0. Model 3 residuals were used as the response varialbe for
Model 4. While the residuals of the new model still failed the Shapiro-Wilk
test and appear to be left-skewed, all predictors were shown to be affecting
*lnrev*. Therefore, we were ready to formulate our new and first full model for testing.

### Model 5

For Model 5, with *lnrev* as the response variable and 0 as the intercept, the
predictors were *budget*, *vote_count*, *original_language*, and the
interaction between *popsqrt* and *genres*.

Although this model produced a high adjusted $R^2$ value of 0.9578, the
residuals of this model did not pass any of the assumption checks.

Additionally, all predictors are shown to be statistically significant.

### Model 6

Finally, a forward stepwise regression analysis was performed to produce a
second model, Model 6, for the final model comparison and selection. Model 6
yielded the exact same results as Model 5. Therefore, there was no need to 
create additional models for comparison. We chose Model 5 as our final model.

## Final Model

With a robust adjusted $R^2$ of 0.9578, the chosen model can explain 95.78% of
the total variance in the sample. It also demonstrates statistical significance
by rejecting the null hypothesis with a $p$-value much below the predetermined significance level $\alpha$ = 0.05.

While the model did not pass any of the assumption checks, it has VIF scores
below 10 or around 10 between all its predictors. Therefore, despite the
shortcomings, we accepted Model 5 as our final model.

For a detailed exploration of the entire model-building process, please refer
to the "Model Building and Diagnostics" section in the Appendix.

# Conclusions and Discussion

Our first hypothesis that different attributes of a feature movie significantly impacts its revenue has been proven correct by our final model. It was also
shown that a movie’s budget and genre contribute significantly, if not more, to
the movie's revenue.

Our model is imperfect despite its high adjusted $R^2$ value and significant predictors. It failed the Shapiro-Wilk test for normality and other assumption
checks. Using the entire data set instead of limiting it to specific genres and languages might have improved the approximation of normality. Time and
knowledge constraints also prevented us from conducting additional, more
detailed analyses. Additionally, the source of the voting data is unknown,
which may introduce bias.

With the above in mind, we propose the following future directions to improve
our model and expand on the studies:

1. Address assumption violations: Perform more transformations on variables or
use more data to achieve normality and constant variance of residuals. A few transformations being considered are logarithmic, square root, and inverse transformations.

2. Alternative modeling approaches: Consider generalized linear models (GLMs)
to accommodate non-normal error distributions or nonlinear relationships.

\newpage

# Appendix

## Data Cleaning

```{r, message=FALSE,warning=FALSE, include=FALSE}
library(tidyverse)
library(lubridate)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(GGally)
library(car)
library(mltools)
library(data.table)
library(fastDummies)
library(lmtest)
library(tidyr)
library(grid)
library(scales)
library(forcats)

original <- read.csv(file.choose(), header = T)
```

```{r, message=FALSE, warning=FALSE}
# subset data
sub1 <- subset(original, select = c("genres", "original_language",
                                    "popularity","budget","revenue",
                                    "runtime","vote_average", "vote_count"))


# replace all empty cells and 0's with NA
sub1[sub1 == ""] = NA
sub1[sub1 == 0] = NA

# clean the data with selected restrictions
sub2 <- sub1 %>%
  na.omit() %>%
  filter(revenue >= 10000) %>%
  filter(budget >= 10000) %>%
  filter(runtime >= 40) %>%
  filter(vote_count >= 95) %>%
  filter(vote_average > 0) 

# split genres and find the top 2 genres
genre <- strsplit(sub2$genres, "-")
sub2$genres <- as.character(lapply(genre, function(x) x[1]))
genre_freq <- table(sub2$genres)
genre_rank <- sort(genre_freq,decreasing = T)
genre_rank[1:2]

# find the top 2 original languages
lang_freq <- table(sub2$original_language)
lang_rank <- sort(lang_freq, decreasing = T)
lang_rank[1:2]

# subset again using only the top 2 genres and the top 2 original languages
# this is the final data set to be used for model building
sub3 <- sub2 %>%
  filter(genres == "Drama" | genres == "Comedy") %>%
  filter(original_language == "en" | original_language == "fr")

# divide revenue and budget by 1000 for easier calculations and visualization
sub3$budget <- as.numeric(as.character(sub3$budget)) / 1000
sub3$revenue <- as.numeric(as.character(sub3$revenue)) / 1000
```

## Exploratory Data Analysis

```{r fig.cap="Histogram of Response Variable", message=FALSE, warning=FALSE}
# check normality of the y values before the analysis
ggplot(sub3, aes(x = revenue)) +
  geom_histogram(fill = "darkblue", color = "white") +
  labs(x = "revenue (in thousand dollars)", y = "count")
```

```{r fig.cap="Histograms and Barplots of Predictor Variables", message=FALSE, warning=FALSE}
g1 <-ggplot(sub3, aes(x = genres)) + 
  geom_bar(fill = "darkblue", color = "white")
g2 <-ggplot(sub3, aes(x = original_language)) + 
  geom_bar(fill = "darkblue", color = "white")
g3 <-ggplot(sub3, aes(x = popularity)) +
  geom_histogram(fill = "darkblue", color = "white", binwidth = 50)
g4 <-ggplot(sub3, aes(x = budget)) + 
  geom_histogram(fill = "darkblue", color = "white")
g5 <-ggplot(sub3, aes(x = runtime)) + 
  geom_histogram(fill = "darkblue", color = "white")
g6 <-ggplot(sub3, aes(x = vote_average)) + 
  geom_histogram(fill = "darkblue", color = "white")
g7<-ggplot(sub3, aes(x = vote_count)) + 
  geom_histogram(fill = "darkblue", color = "white")

grid.arrange(g1, g2, g3, g4, g5, g6, g7, ncol = 2)
```

```{r fig.cap="Predictor vs. Response", message=FALSE, warning=FALSE}
# create scatter plots between each predictor var and the response var
s.popularity <- ggplot(sub3, aes(x = popularity, y = revenue)) +
  geom_point(color = "darkblue") 
s.budget <- ggplot(sub3, aes(x = budget, y = revenue)) +
  geom_point(color = "darkblue") 
s.runtime <- ggplot(sub3, aes(x = runtime, y = revenue)) +
  geom_point(color = "darkblue") 
s.vote_average <- ggplot(sub3, aes(x = vote_average, y = revenue)) +
  geom_point(color = "darkblue") 
s.vote_count <- ggplot(sub3, aes(x = vote_count, y = revenue)) +
  geom_point(color = "darkblue") 

# create boxplots between each categorical predictor and the response
box.genres <- ggplot(sub3, aes(x = genres, y = revenue)) +
           geom_boxplot(color = "darkblue", outlier.color = "red",
                        outlier.shape = 4, outlier.size = 2) 
box.original_language <- ggplot(sub3, aes(x = original_language, y = revenue)) +
              geom_boxplot(color = "darkblue", outlier.color = "red",
                           outlier.shape = 4, outlier.size = 2) 

# divide the frame in grid using grid.arrange() and put above plots in it
grid.arrange(s.popularity, s.budget, s.runtime, s.vote_average, s.vote_count,
             box.genres, box.original_language, ncol = 4, nrow = 2)
```

```{r fig.cap="Initial Pairwise Comparison", message=FALSE, warning=FALSE}
ggpairs(sub3) + theme(axis.text.x = element_text(size= 2, angle = 90),
                      axis.text.y = element_text(size = 2),
                      strip.text.y = element_text(angle = 0))
```

## Forward Stepwise Regression

```{r, message=FALSE, warning=FALSE}
# perform stepwise regression using the original data set

# specify the null model with no predictor
original.null <- lm(revenue ~ 1, data = sub3)
# specify the full model using all of the potential predictors
original.full <- lm(revenue ~ ., data = sub3)
# use the stepwise algorithm to build a parsimonious model
original.step <- step(original.null,
                      scope = list(lower = original.null,
                                   upper = original.full),
                      direction = "both", test = "F")

summary(original.step)
```

According to the final output of the stepwise regression model analysis, the
significant predictors are *vote_count*, *budget*, *popularity*, and *genres*.
Predictors *vote_average* and *runtime* are not statistically significant and
have been excluded from the stepwise model.

## Residual Diagnostics

```{r fig.cap="Residual Diagnostics: Linearity", message=FALSE, warning=FALSE}
# perform residual diagnostics on the residuals of the final stepwise model

# check linearity
ggplot(data = original.step, aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.5, color = "darkblue") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Predicted Values", y = "Residuals")
# predictor: vote count
l.vc <- ggplot(data = original.step, aes(x = vote_count, y = .resid)) +
  geom_point(alpha = 0.5, color = "darkblue") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "vote count", y = "Residuals")
# predictor: budget
l.budget <- ggplot(data = original.step, aes(x = budget, y = .resid)) +
  geom_point(alpha = 0.5, color = "darkblue") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "budget (in thousand $)", y = "Residuals")
# predictor: popularity
l.popularity <- ggplot(data = original.step, aes(x = popularity, y = .resid)) +
  geom_point(alpha = 0.5, color = "darkblue") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "popularity", y = "Residuals")

# divide the frame in grid using grid.arrange() and put above plots in it
grid.arrange(l.vc, l.budget, l.popularity, ncol = 3, nrow = 1)
```

Only *popularity* passed the linearity check as it showed no distinct pattern.

```{r fig.cap="Residual Diagnostics: Constant Variance", message=FALSE, warning=FALSE}
# perform residual diagnostics on the residuals of the final stepwise model

# check constant variance
bptest(original.step)
```

Because the $p$-value < 2.2$e^{-16}$ is much less than the significance level
$\alpha$ = 0.05, we can reject the null hypothesis. Therefore, there is
sufficient evidence to conclude the residuals do not have constant variance. 

```{r fig.cap="Residual Diagnostics: Normality", message=FALSE, warning=FALSE}
# perform residual diagnostics on the residuals of the final stepwise model

# check normality 
ggplot(data = original.step, aes(x = .resid)) + 
  geom_histogram(fill = "darkblue", color = "white") +
  labs(x = "revenue (in thousand $)", y = "count")

ggplot(data = original.step, aes(x = .resid)) + 
  geom_boxplot(color = "darkblue", outlier.color = "red",
               outlier.shape = 4, outlier.size = 2) +
           labs(y = "revenue (in thousand $)")

qqnorm(resid(original.step))
qqline(resid(original.step))

shapiro.test(resid(original.step))
```

The residuals do not appear to be normally distributed due to the following
reasons: 

1. The histogram is slightly right-skewed instead of bell-shaped.
2. The mean is not centered at 0 in the boxplot.
3. The right tail of the Q-Q plot deviates significantly from the fitted Q-Q
line.
4. The Shapiro-Wilk test $p$-value < 2.2$e^{-16}$ is much less than the
significance level $\alpha$ = 0.05, suggesting that I can reject the null
hypothesis that the residuals are normally distributed.

```{r, message=FALSE, warning=FALSE}
# perform residual diagnostics on the residuals of the final stepwise model

# check independence
durbinWatsonTest(original.step)
```

Because the $p$-value = 0.608 is greater than the significance level $\alpha$ =
0.05, I cannot reject the null hypothesis. Therefore, there is insufficient
evidence to conclude that the residuals are not independent.

## Model Building and Diagnostics

```{r, message=FALSE, warning=FALSE}
rev <- sub3$revenue
vc <- sub3$vote_count
budget <- sub3$budget
pop <- sub3$popularity
va <- sub3$vote_average
rt <- sub3$runtime

# create dummy variables for each cat var
genre <- ifelse(sub3$genres == "Drama", 1, 0)
o_lang <- ifelse(sub3$original_language == "en", 1, 0)
```

```{r fig.cap="Initial Plots of Predictor vs. Response Variables", message=FALSE, warning=FALSE}
# check the relationship between each predictor var and rev
a <- ggplot(sub3, aes(x = budget, y = rev)) +
  geom_point(color = "darkblue") +
  labs(x = "budget (in thousand $)", y = "revenue (in thousand $)")
b <- ggplot(sub3, aes(x = runtime, y = rev)) +
  geom_point(color = "darkblue") +
  labs(x = "runtime (in minutes)", y = "revenue (in thousand $)")
c <- ggplot(sub3, aes(x = popularity, y = rev)) +
  geom_point(color = "darkblue") +
  labs(x = "popularity", y = "revenue (in thousand $)")
d <- ggplot(sub3, aes(x = vote_count, y = rev)) +
  geom_point(color = "darkblue") +
  labs(x = "vc", y = "revenue (in thousand $)")
e <- ggplot(sub3, aes(x = vote_average, y = rev)) +
  geom_point(color = "darkblue") +
  labs(x = "va", y = "revenue (in thousand $)")
f <- ggplot(sub3, aes(x = genres, y = rev)) +
  geom_point(color = "darkblue") +
  labs(x = "genres", y = "revenue (in thousand $)")
g <- ggplot(sub3, aes(x = original_language, y = rev)) +
  geom_point(color = "darkblue") +
  labs(x = "original lang", y = "revenue (in thousand $)")

# divide the frame in grid using grid.arrange() and put above plots in it
grid.arrange(a, b, c, d, e, f, g, ncol = 3, nrow = 3)
```

### Model Building

```{r fig.cap="Interaction Between popularity and genres", message=FALSE, warning=FALSE}
# check interaction between popularity and genre
plot(pop, rev, type = "n")
points(pop[genre == 0], rev[genre == 0], col="darkblue")
points(pop[genre == 1], rev[genre == 1], col="darkred")
```

The two variables are overlapping, implying an interaction between the two. 

```{r fig.cap="Residual Diagnostics: y1", message=FALSE, warning=FALSE}
lm1 <- lm(rev ~ 0 + pop * genre)
y1 <- lm1$residuals

ggplot(sub3, aes(x = y1))+
  geom_histogram(fill = "darkblue", color = "white") +
  labs(x = "revenue (in thousand $)", y = "count")
qqnorm(y1)
qqline(y1)
shapiro.test(y1)
```

```{r fig.cap="Plots of Predictors vs. y1", message=FALSE, warning=FALSE}
# check the relationship between each predictor variable and y1
a <- ggplot(sub3, aes(x = budget, y = y1)) +
  geom_point(color = "darkblue") +
  labs(x = "budget (in thousand $)", y = "revenue (in thousand $)")
b <- ggplot(sub3, aes(x = runtime, y = y1)) +
  geom_point(color = "darkblue") +
  labs(x = "runtime (in minutes)", y = "revenue (in thousand $)")
c <- ggplot(sub3, aes(x = popularity, y = y1)) +
  geom_point(color = "darkblue") +
  labs(x = "popularity", y = "revenue (in thousand $)")
d <- ggplot(sub3, aes(x = vote_count, y = y1)) +
  geom_point(color = "darkblue") +
  labs(x = "vc", y = "revenue (in thousand $)")
e <- ggplot(sub3, aes(x = vote_average, y = y1)) +
  geom_point(color = "darkblue") +
  labs(x = "va", y = "revenue (in thousand $)")
f <- ggplot(sub3, aes(x = genres, y = y1)) +
  geom_point(color = "darkblue") +
  labs(x = "genres", y = "revenue (in thousand $)")
g <- ggplot(sub3, aes(x = original_language, y = y1)) +
  geom_point(color = "darkblue") +
  labs(x = "original lang", y = "revenue (in thousand $)")

# divide the frame in grid using grid.arrange() and put above plots in it
grid.arrange(a, b, c, d, e, f, g, ncol = 3, nrow = 3)
```

Model 1 was fitted with an interaction between *popularity* and *generes*, with
an intercept set to 0. 

As shown by the results, Model 1 does not pass the Shapiro-Wilk normality test
and appears right-skewed.

According to the scatter plots for each individual predictor variable against
the Model 1 residuals, most predictors exhibited patterns except for *budget*,
*popularity*, and *vote_count*. These three variables will be further observed
before we decide whether to exclude them in the model.

```{r fig.cap="Residual Diagnostics: y2", message=FALSE, warning=FALSE}
lm2 <- lm(y1 ~ 0 + budget)
y2 <- lm2$residuals

ggplot(sub3, aes(x = y2))+
  geom_histogram(fill = "darkblue", color = "white") +
  labs(x = "revenue (in thousand $)", y = "count")
qqnorm(y2)
qqline(y2)
shapiro.test(y2)
```

```{r fig.cap="Plots of Predictors vs. y2", message=FALSE, warning=FALSE}
# check the relationship between each predictor variable and y1
a <- ggplot(sub3, aes(x = budget, y = y2)) +
  geom_point(color = "darkblue") +
  labs(x = "budget (in thousand $)", y = "revenue (in thousand $)")
b <- ggplot(sub3, aes(x = runtime, y = y2)) +
  geom_point(color = "darkblue") +
  labs(x = "runtime (in minutes)", y = "revenue (in thousand $)")
c <- ggplot(sub3, aes(x = popularity, y = y2)) +
  geom_point(color = "darkblue") +
  labs(x = "popularity", y = "revenue (in thousand $)")
d <- ggplot(sub3, aes(x = vote_count, y = y2)) +
  geom_point(color = "darkblue") +
  labs(x = "vc", y = "revenue (in thousand $)")
e <- ggplot(sub3, aes(x = vote_average, y = y2)) +
  geom_point(color = "darkblue") +
  labs(x = "va", y = "revenue (in thousand $)")
f <- ggplot(sub3, aes(x = genres, y = y2)) +
  geom_point(color = "darkblue") +
  labs(x = "genres", y = "revenue (in thousand $)")
g <- ggplot(sub3, aes(x = original_language, y = y2)) +
  geom_point(color = "darkblue") +
  labs(x = "original lang", y = "revenue (in thousand $)")

# divide the frame in grid using grid.arrange() and put above plots in it
grid.arrange(a, b, c, d, e, f, g, ncol = 3, nrow = 3)
```

Model 2 was fitted with *budget*, with an intercept set to 0. 

Like Model 1, Model 2 does not pass the Shapiro-Wilk normality test and appears right-skewed.

The scatter plots still show that *budget*, *popularity*, and *vote_count* to
have no effect on the response variable Moreover, *genres* seems to have become constant as well. Since *budget* and *genres* are the two variables that we
wanted to focus on as stated in our hypothesis, we have decided to perform a logarithmic transformation on *revenue* to see if this will make a difference. 

```{r fig.cap="Plots of Predictor vs. lnrev", message=FALSE, warning=FALSE}
# perform log transformation on revenue and edit the original data set to
# exclude revenue and include lnrev
lnrev <- log(sub3$revenue)
sub3$lnrev <- lnrev
sub3 <- sub3[, -5]

# convert cat vars genres and original language to be binary data
sub3$genres <- genre
sub3$original_language <- o_lang

# check the relationship between each predictor variable and lnrev
a <- ggplot(sub3, aes(x = budget, y = lnrev)) +
  geom_point(color = "darkblue") +
  labs(x = "budget (in thousand $)", y = "lnrev (in thousand $)")
b <- ggplot(sub3, aes(x = runtime, y = lnrev)) +
  geom_point(color = "darkblue") +
  labs(x = "runtime (in minutes)", y = "lnrev (in thousand $)")
c <- ggplot(sub3, aes(x = popularity, y = lnrev)) +
  geom_point(color = "darkblue") +
  labs(x = "popularity", y = "lnrev (in thousand $)")
d <- ggplot(sub3, aes(x = vote_count, y = lnrev)) +
  geom_point(color = "darkblue") +
  labs(x = "vc", y = "lnrev (in thousand $)")
e <- ggplot(sub3, aes(x = vote_average, y = lnrev)) +
  geom_point(color = "darkblue") +
  labs(x = "va", y = "lnrev (in thousand $)")
f <- ggplot(sub3, aes(x = genres, y = lnrev)) +
  geom_point(color = "darkblue") +
  labs(x = "genres", y = "lnrev (in thousand $)")
g <- ggplot(sub3, aes(x = original_language, y = lnrev)) +
  geom_point(color = "darkblue") +
  labs(x = "original lang", y = "lnrev (in thousand $)")

# divide the frame in grid using grid.arrange() and put above plots in it
grid.arrange(a, b, c, d, e, f, g, ncol = 3, nrow = 3)
```

```{r fig.cap="Interaction Between popularity and genres", message=FALSE, warning=FALSE}
# check interaction between pop and genre
plot(pop, lnrev, type = "n")
points(pop[genre == 0], lnrev[genre == 0], col="darkblue")
points(pop[genre == 1], lnrev[genre == 1], col="darkred")
```

Similar to the results from before, the two variables overlap, suggesting a
strong interaction between them.

```{r fig.cap="Residual Diagnostics: y1", message=FALSE, warning=FALSE}
lm1 <- lm(lnrev ~ 0 + pop * genre)
y1 <- lm1$residuals

ggplot(sub3, aes(x = y1))+
  geom_histogram(fill = "darkblue", color = "white") +
  labs(x = "lnrev (in thousand $)", y = "count")
qqnorm(y1)
qqline(y1)
shapiro.test(y1)
```

```{r fig.cap="Plots of Predictors vs. y1", message=FALSE, warning=FALSE}
# check the relationship between each predictor variable and y1
a <- ggplot(sub3, aes(x = budget, y = y1)) +
  geom_point(color = "darkblue") +
  labs(x = "budget (in thousand dollars)", y = "lnrev (in thousand $)")
b <- ggplot(sub3, aes(x = runtime, y = y1)) +
  geom_point(color = "darkblue") +
  labs(x = "runtime (in minutes)", y = "lnrev (in thousand $)")
c <- ggplot(sub3, aes(x = popularity, y = y1)) +
  geom_point(color = "darkblue") +
  labs(x = "popularity", y = "lnrev (in thousand $)")
d <- ggplot(sub3, aes(x = vote_count, y = y1)) +
  geom_point(color = "darkblue") +
  labs(x = "vc", y = "lnrev (in thousand $)")
e <- ggplot(sub3, aes(x = vote_average, y = y1)) +
  geom_point(color = "darkblue") +
  labs(x = "va", y = "lnrev (in thousand $)")
f <- ggplot(sub3, aes(x = genres, y = y1)) +
  geom_point(color = "darkblue") +
  labs(x = "genres", y = "lnrev (in thousand $)")
g <- ggplot(sub3, aes(x = original_language, y = y1)) +
  geom_point(color = "darkblue") +
  labs(x = "original lang", y = "lnrev (in thousand $)")

# divide the frame in grid using grid.arrange() and put above plots in it
grid.arrange(a, b, c, d, e, f, g, ncol = 3, nrow = 3)
```

The new Model 1 was fitted with an interaction between *popularity* and
*genres*, with an intercept set to 0. 

It does not pass the Shapiro-Wilk normality test and now appears slightly
left-skewed.

All variables now appear to impact *lnrev* except *vote_average*. This means
that our transformation has made a difference. We will continue to monitor
*vote_average* before deciding whether or not to exclude it from the model.

```{r fig.cap="Residual Diagnostics: y2", message=FALSE, warning=FALSE}
lm2 <- lm(y1 ~ 0 + budget)
y2 <- lm2$residuals

ggplot(sub3, aes(x = y2))+
  geom_histogram(fill = "darkblue", color = "white") +
  labs(x = "lnrev (in thousand $)", y = "count")
qqnorm(y2)
qqline(y2)
shapiro.test(y2)
```

```{r fig.cap="Plots of Predictors vs. y2", message=FALSE, warning=FALSE}
# check the relationship between each predictor variable and y2
a <- ggplot(sub3, aes(x = budget, y = y2)) +
  geom_point(color = "darkblue") +
  labs(x = "budget (in thousand $)", y = "lnrev (in thousand $)")
b <- ggplot(sub3, aes(x = runtime, y = y2)) +
  geom_point(color = "darkblue") +
  labs(x = "runtime (in minutes)", y = "lnrev (in thousand $)")
c <- ggplot(sub3, aes(x = popularity, y = y2)) +
  geom_point(color = "darkblue") +
  labs(x = "popularity", y = "lnrev (in thousand $)")
d <- ggplot(sub3, aes(x = vote_count, y = y2)) +
  geom_point(color = "darkblue") +
  labs(x = "vc", y = "lnrev (in thousand $)")
e <- ggplot(sub3, aes(x = vote_average, y = y2)) +
  geom_point(color = "darkblue") +
  labs(x = "va", y = "lnrev (in thousand $)")
f <- ggplot(sub3, aes(x = genres, y = y2)) +
  geom_point(color = "darkblue") +
  labs(x = "genres", y = "lnrev (in thousand $)")
g <- ggplot(sub3, aes(x = original_language, y = y2)) +
  geom_point(color = "darkblue") +
  labs(x = "original lang", y = "lnrev (in thousand $)")

# divide the frame in grid using grid.arrange() and put above plots in it
grid.arrange(a, b, c, d, e, f, g, ncol = 3, nrow = 3)
```

The new Model 2 was fitted with a single variable *budget* and a 0 intercept.

It still does not pass the Shapiro-Wilk normality test and is slightly
left-skewed.

*vote_average* again exhibits no pattern, implying it does affect *lnrev*, and *runtime* now looks more constant than before. All the other variables seem
to be impacting y.

```{r fig.cap="Residual Diagnostics: y3", message=FALSE, warning=FALSE}
lm3 <- lm(y2 ~ 0 + vc)
y3 <- lm3$residuals

ggplot(sub3, aes(x = y3))+
  geom_histogram(fill = "darkblue", color = "white") +
  labs(x = "lnrev (in thousand $)", y = "count")
qqnorm(y3)
qqline(y3)
shapiro.test(y3)
```

```{r fig.cap="Plots of Predictors vs. y3", message=FALSE, warning=FALSE}
# check the relationship between each predictor variable and y3
a <- ggplot(sub3, aes(x = budget, y = y3)) +
  geom_point(color = "darkblue") +
  labs(x = "budget (in thousand $)", y = "lnrev (in thousand $)")
b <- ggplot(sub3, aes(x = runtime, y = y3)) +
  geom_point(color = "darkblue") +
  labs(x = "runtime (in minutes)", y = "lnrev (in thousand $)")
c <- ggplot(sub3, aes(x = popularity, y = y3)) +
  geom_point(color = "darkblue") +
  labs(x = "popularity", y = "lnrev (in thousand $)")
d <- ggplot(sub3, aes(x = vote_count, y = y3)) +
  geom_point(color = "darkblue") +
  labs(x = "vc", y = "lnrev (in thousand $)")
e <- ggplot(sub3, aes(x = vote_average, y = y3)) +
  geom_point(color = "darkblue") +
  labs(x = "va", y = "lnrev (in thousand $)")
f <- ggplot(sub3, aes(x = genres, y = y3)) +
  geom_point(color = "darkblue") +
  labs(x = "genres", y = "lnrev (in thousand $)")
g <- ggplot(sub3, aes(x = original_language, y = y3)) +
  geom_point(color = "darkblue") +
  labs(x = "original lang", y = "lnrev (in thousand $)")

# divide the frame in grid using grid.arrange() and put above plots in it
grid.arrange(a, b, c, d, e, f, g, ncol = 3, nrow = 3)
```

Model 3 was fitted with *vote_count* and an intercep of 0. 

Like the previous two models, this model is left-skewed and does not pass the Shapiro-Wilk test.

We will be taking out *vote_average* and *runtime*, with the latter collapsing
more, as these two look constant compared to all other variables.

Before continuing further analysis, we have decided to transform *popularity*
with a square root transformation for easier computation and model fitting and
hoping this will help us pass the normality assumption check.

```{r fig.cap="Residual Diagnostics: y4", message=FALSE, warning=FALSE}
# perform square root transformation on popularity and edit the data set to
# exclude popularity and include popsqrt
popsqrt <- sqrt(pop)
sub3$popsqrt <- popsqrt
sub3 <- sub3[, -3]

# fit the model using the interaction between popsqrt and genres
lm4 <- lm(y3 ~ 0 + popsqrt * genre)
y4 <- lm4$residuals

ggplot(sub3, aes(x = y4))+
  geom_histogram(fill = "darkblue", color = "white") +
  labs(x = "lnrev (in thousand $)", y = "count")
qqnorm(y4)
qqline(y4)
shapiro.test(y4)
```

```{r fig.cap="Plots of Predictors vs. y4", message=FALSE, warning=FALSE}
# check the relationship between each predictor variable and y4
a <- ggplot(sub3, aes(x = budget, y = y4)) +
  geom_point(color = "darkblue")+
  labs(x = "budget (in thousand $)", y = "lnrev (in thousand $)")
b <- ggplot(sub3, aes(x = popsqrt, y = y4)) +
  geom_point(color = "darkblue") +
  labs(x = "popularity", y = "lnrev (in thousand $)")
c <- ggplot(sub3, aes(x = vote_count, y = y4)) +
  geom_point(color = "darkblue") +
  labs(x = "vc", y = "lnrev (in thousand $)")
d <- ggplot(sub3, aes(x = genres, y = y3)) +
  geom_point(color = "darkblue") +
  labs(x = "genres", y = "lnrev (in thousand $)")
e <- ggplot(sub3, aes(x = original_language, y = y3)) +
  geom_point(color = "darkblue") +
  labs(x = "original lang", y = "lnrev (in thousand $)")

# divide the frame in grid using grid.arrange() and put above plots in it
grid.arrange(a, b, c, d, e, ncol = 3, nrow = 2)
```

Model 3 residuals were used as my response for Model 4, which incorporated the
interaction between *popsqrt* and *genres* with a 0 intercept.

While the residuals of the new model still failed the Shapiro-Wilk test and
appear to be left-skewed, all predictors were shown to be affecting *lnrev*.

```{r fig.cap="Residual Diagnostics: lm5", message=FALSE, warning=FALSE}
# fit first full model
lm5 <- lm(lnrev ~ 0 + budget + popsqrt * genre + o_lang + vc)
summary(lm5)

# check the residual assumptions
ggplot(sub3, aes(x = lm5$residuals))+
  geom_histogram(fill = "darkblue", color = "white") +
  labs(x = "lnrev (in thousand $)", y = "count")
qqnorm(lm5$residuals)
qqline(lm5$residuals)
ggplot(sub3, aes(x = lm5$fitted.values, y = lm5$residuals)) +
  geom_point(color = "darkblue") +
  labs(x = "Fitted Values", y = "Residuals")

shapiro.test(lm5$residuals)
durbinWatsonTest(lm5)
bptest(lm5)
```

Although this model produced a high adjusted $R^2$ value of 0.9578, the
residuals of this model did not pass the normality check, did not exhibit
significant autocorrelation, and lacked homoscedasticity.

All predictors are shown to be statistically significant.

```{r fig.cap="Pairwise Comparison for Model 4", message=FALSE, warning=FALSE}
# edit the data set to exclude runtime and vote average
sub3 <- sub3[, -c(4, 5)]

# create pairwise scatter plots and coefficient of correlation
ggpairs(sub3) + theme(axis.text.x = element_text(size= 2.5, angle = 90),
                      axis.text.y = element_text(size = 2.5),
                      strip.text.y = element_text(angle = 0))
```

Similar to the results of our initial pairwise comparison, majority of the
predictor variables do not exhibit multicollinearity. An intermediate
correlation between *budget* and *lnrev* is expected, as this is one of the
variables we wanted to take a closer look at and one of our hypotheses.

```{r, message=FALSE, warning=FALSE}
# perform forward stepwise regression and use this model as Model 6

# specify a null model with no predictors
null.model <- lm(lnrev ~ 0, data = sub3)

# use a stepwise algorithm to build a parsimonious model
# null to full
lm6 <- step(null.model,
            scope = list(lower = null.model, upper = lm5),
            direction = "both", test = "F")
summary(lm6)

# check the residual assumptions
ggplot(sub3, aes(x = lm6$residuals))+
  geom_histogram(fill = "darkblue", color = "white") +
  labs(x = "lnrev (in thousand $)", y = "count")
qqnorm(lm6$residuals)
qqline(lm6$residuals)
ggplot(sub3, aes(x = lm6$fitted.values, y = lm6$residuals)) +
  geom_point(color = "darkblue") +
  labs(x = "Fitted Values", y = "Residuals")

shapiro.test(lm6$residuals)
durbinWatsonTest(lm6)
bptest(lm6)
```

Model 6 yielded the exact same results as Model 5. Therefore, there's no need
to do further model comparison, and we choose Model 5 as our final model.

### Final Model Diagnostics

```{r, message=FALSE, warning=FALSE}
final.model <- lm(lnrev ~ 0 + budget + popsqrt * genre + o_lang + vc)

# non-significant F-tests with overall significant F-test
summary(final.model)

# variance inflation factor (VIF)
vif(final.model)

# check the residual assumptions
ggplot(sub3, aes(x = final.model$residuals))+
  geom_histogram(fill = "darkblue", color = "white") +
  labs(x = "lnrev (in thousand $)", y = "count")

qqnorm(final.model$residuals)
qqline(final.model$residuals)
ggplot(sub3, aes(x = final.model$fitted.values, y = final.model$residuals)) +
  geom_point(color = "darkblue") +
  labs(x = "Fitted Values", y = "Residuals")

shapiro.test(final.model$residuals)
durbinWatsonTest(final.model)
bptest(final.model)
```

The final model has a robust adjusted $R^2$ value of 0.9578. It also
demonstrates statistical significance by rejecting my null hypothesis with a
$p$-value much below the predetermined significance level $\alpha$ = 0.05.

However, our final model did not pass the normality or the constant variance
checks. The model also has VIF scores below 10 between its predictors. This
also corresponds to the results of the pairwise comparisons.

In conclusion, regardless of its shortcomings, we accept Model 5 as our final
model.

# Differential Privacy

```{r, message=FALSE,warning=FALSE, include=FALSE}
library(diffpriv)
```

```{r}
set.seed(123)

# Define target function for linear regression coefficients with intercept
target_function <- function(X) {
  model_sim <- lm(lnrev ~ budget + popsqrt * genre + o_lang + vc, data = X)
  return(coef(model_sim))  # Returns coefficients including the intercept
}

count(sub3)
# Define the Laplace mechanism
mechanism <- DPMechLaplace(target = target_function, sensitivity = 1 / 2624,
                           dims = 7)

# Set privacy parameters
pparams_sim <- DPParamsEps(epsilon = 1)

# Generate private response for one run (example)
private_response <- releaseResponse(mechanism, privacyParams = pparams_sim, X = sub3)

# Display non-private and private coefficients for comparison
cat("Non-private coefficients:\n", coef(lm(lnrev ~ budget + popsqrt * genre + o_lang + vc, data = sub3)), "\n")
cat("Private coefficients:\n", private_response$response, "\n")
```

```{r}
# Run simulations to get private coefficients
set.seed(1000)  # For reproducibility
n_simulations <- 10000
private_coefficients <- replicate(
  n_simulations,
  releaseResponse(mechanism, privacyParams = pparams_sim, X = sub3)$response)

# Get non-private coefficients
non_private_coeffs <- coef(lm(lnrev ~ budget + popsqrt * genre + o_lang + vc, data = sub3))

# Create a data frame for private coefficients
private_df <- as.data.frame(t(private_coefficients))
colnames(private_df) <- names(non_private_coeffs)

# Reshape data for plotting
private_df$Simulation <- 1:n_simulations
private_long <- pivot_longer(private_df, cols = -Simulation, names_to = "Coefficient", values_to = "Value")

# Prepare non-private coefficients for plotting
non_private_df <- data.frame(
  Coefficient = names(non_private_coeffs),
  Value = as.numeric(non_private_coeffs)
)

# Plot private vs. non-private coefficients
ggplot() +
  geom_density(data = private_long, aes(x = Value, fill = "Private"), alpha = 0.5) +
  geom_vline(data = non_private_df, aes(xintercept = Value, linetype = "Non-Private"), color = "black", size = 0.5) +
  facet_wrap(~ Coefficient, scales = "free") +
  labs(title = "Distributions of Private vs. Non-Private Coefficients",
       x = "Coefficient Value",
       y = "Density",
       fill = "Type",
       linetype = "Type") +
  scale_fill_manual(values = c("Private" = "red")) +
  scale_linetype_manual(values = c("Non-Private" = "dashed")) +
  theme_minimal()

# Validate means of private and non-private coefficients
cat("Mean of private budget coefficient:\n", mean(private_df$genre), "\n")
cat("Non-private budget coefficient:\n", non_private_coeffs["genre"], "\n")
```